---
title: "The Distributed Schema"
date: 2021-01-31T18:52:55-08:00
draft: true
---


## The Problem: Data is hopelessly fragmented (along with its corresponding schemata)

The currently available data formats widely used for application -> application communication over a network are insufficient solutions for their stated task, especially in the case of interfaces for public integrations, and the ad-hoc tooling meant to paper over these inadequacies by providing verification and documented semantics do not fully solve the problem.  As the sheer quantity of machine-readable data passed between interconnected web services, both public and private, continues to increase at a dramatic pace, the need for a means to freely manipulate data from a variety of sources will become more urgent.

### _But first a prelude_

In the early phases of the World Wide Web, URI-addressable content was delivered directly to end-users from centralized servers over HTTP, and individual pieces of content could be associated with one another via [hyperlinks](https://www.w3.org/MarkUp/html-spec/html-spec_7.html).  URI's could be used to denote a particular _resource_, as well as its constituent parts, accessible via HTTP.

Less than a decade later, application-level protocols for transmitting machine-readable data took shape in the form of XML-RPC (which evolved into SOAP) and the XMLHttpRequest, all of which leveraged HTTP as a transport mechanism.   Interestingly, SOAP and its associated XML payloads could be imprinted with a verifiable formal structure, viz., an XML Schema.  Such structures aimed to facilitate interpreting a given set of information expressed in XML according to some defined schema by both machines and humans.

> An XSD schema is a set of components such as type definitions and element declarations. These can be used to assess the validity of well-formed element and attribute information items (as defined in [XML Infoset]), and furthermore to specify additional information about those items and their descendants. **These augmentations to the information set make explicit information that was implicitly present in the original document** (or in the original document and the governing schema, taken together), such as normalized and/or default values for attributes and elements and the types of element and attribute information items. The input information set is also augmented with information about the validity of the item, or about other properties described in this specification. 

> ~ _from [the w3c documentation](https://www.w3.org/TR/xmlschema11-1/#xsover) on XMLSchema_; 
> emphasis added

Over and above verifying structural correctness (or fitness for processing by a particular consumer), XML Schema was designed to imbue the components of a data document with definite meanings that delimited and conveyed how such components could be used.  Schemas can be built with XML and published online, thus ensuring that the schema itself and the semantics it defines can be accessed just like any other resource.  These early efforts ensured that the semantic information for a particular avenue of network communication can be shared as an artifact to be used by humans to understand the desired data format as well as by machines for automated verification and the generation of entailed information.  It shouldn't be too difficult to note the parallels to modern JSON schema tooling, e.g., [Swagger](https://swagger.io/), or the automated generation of client-server modules for a particular message contract based on an abstract data definition language, e.g., [gRPC](https://grpc.io/docs/what-is-grpc/introduction/).

Despite a cosmetic similarity, our present data format boondoggle and its distant XML ancestor are not related by a continuous lineage.  XML, by most accounts, did not [spark joy](https://www.youtube.com/watch?v=9AvWs2X-bEA).  The schema definition language was unnecessarily complex, full of surprising inconsistencies, as well as unwieldy to work with by both machines and humans.  As a consequence, the task of transmitting structured information over a network was stripped down to its barest essentials and celebrated as a liberating force with little to no constraining rules beyond its syntax.  In other words, JSON was very Punk Rockâ„¢. 

And just as the bacchic extreme of Punk culture's leap in the direction absolute freedom in the form of Anarchy sunk back into an contradiction-riddled "code" of behavioral prohibitions (signing to a commercial label), the initial high of conveying data with the relatively primitive structures in JSON quickly led back to a disorienting thicket of suddenly critical parameter names and value types, which subsequently was addressed using bolted on verification techniques or copious documentation.  In the decades long attempt to calibrate our data formats to be useful for interpretation and verification, there must be a better solution than the verbose and complex XML Schema and the anarchic JSON + tooling.  gRPC appears in this spectrum somewhere towards XML-Schema, but it brings along the baggage of needing a hefty bit of additional tooling to use.

GraphQL is fantastic in this regard.  Its schemata can be expressed in freely available artifacts, easily consumed by humans and machines.  Verification and additional tooling are present but not burdensome; GraphQL request payloads can be constructed with relatively simple structured strings, and clients can declaratively express the precise structure and content of a desired response.  If verification, light-weight tooling, and machine-processable schema-as-artifact representations were the primary targets of data formats we might be at our logical conclusion.  Alas.

#### Back to the problem

Recall the earlier focus on drawing out implicit term meanings from the documentation for XML Schema.  Verification and tooling are a bonus, but the crux of the matter lies in semantics and entailed properties.  The form of a particular GraphQL schema is peculiar to its underlying content; GraphQL is a generic means of exchange for bespoke heaps of data/services.  To be fair, the earlier XML Schema technology did little to unify the semantic meanings of document components across schema instances, but the current absence of this priority is notable.  For an industry that prizes abstraction and re-use, we might puzzle why these treasured principles have not extended further into the domain of data semantics, and instead have tarried with syntax or parsing/serialization.

- If ordered and key/value pair collections can be nested inside one another to an arbitrary depth, might we ask whether it is possible to offer the same composability when it comes to the semantic schemata that ostensibly govern how this serialized data is meant to be used?  

- When schema artifacts can be published via web services, shouldn't they also be related to one another via URI's to facilitate authoritative records and their re-use?

- Does the wide availability of language agnostic formats as well as tooling for converting data from one syntax to another also prompt the question of whether converting between similar but cosmetically divergent semantic units could be automated?  Could the rule for such a conversion be expressed in a semantic schema represented by a machine-readable artifact?


Or we could just document _the hell_ out of every public web service endpoint and manually build clients and parsing bridges to amalgamate every island of data into a uniformly queryable composite.  What if it were possible to declaratively express an abstract rule set for merging together api responses from Github, Google, Facebook, and a local chess club, _and querying the resultant dataset_ in a single client request?  The ability to define a `VIEW` in SQL from diverse data sources, potentially involving one or more projections, within the regular structures of the query language that emits a component which behaves in an orderly fashion is an attractive analogue; however, in the SQL example a data schema is assumed to be present out the outset, whereas in the field of web service API's no such schema is present or could be hoped for.  Fortunately, the web offers a precedent for navigating a fluid and un-manageable territory in the form of DNS.  No one data source authoritatively determines a mapping between human-readable URI's and their destination IP addresses; further, the means of discovering URI's is not driven by a telephone book-type procedure where the target must be known in advance.  HTTP documents are related via hyperlinks, and hyperlinks can be defined in an ongoing manner by TLD providers.  

Perhaps there exists a path through the semantic forest whereby something akin to a SQL `VIEW` drawing upon diverse data sources (in this case data schemata) can be constructed within a distributed system for managing such schemata in order to manipulate heterogeneous clumps of information _that were never designed to work together in the first place_ and without an over-arching plan for how to enable this level of coordination.

